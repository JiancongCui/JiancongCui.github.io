<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="JCC"><title>torch.scatter · Jiancong Cui</title><meta name="description" content="Tips
var.item() - 如果张量 var 中只有一个元素，使用该方法取出该元素
属性
tensor.size - shapes
tensor的拼接
tensor的拼接操作经常用到、特地将其整理如下

cat - dim=all
stack - dim=new_dim
dstack - d"><meta name="keywords" content="Computer Science, Privacy Security,"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&amp;display=swap"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/tag.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><link rel="shortcut icon" href="/images/jcc.webp"><script src="/js/jquery.js"></script><!-- 主题：可换 one-dark/tomorrow/github 等 -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prism-themes@1.9.0/themes/prism-one-dark.css">
<!-- 行号 + 工具栏的样式 -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/plugins/toolbar/prism-toolbar.css">
<script defer src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs/plugins/autoloader/prism-autoloader.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs/plugins/toolbar/prism-toolbar.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="page-top animated fadeInDown"><div class="nav-container"> <div class="nav"><a class="nav-item logo" href="/">Jiancong Cui </a><a class="nav-item" href="/">About Me</a><a class="nav-item" href="/blogs">Posts</a><a class="nav-item" href="/archives">Archive</a><a class="nav-item" href="/tags">Tags</a><a class="nav-item" href="/quotes">Quotes</a></div></div></div><div id="main-container"><div class="main-content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><a>torch.scatter</a></div><div class="post-labels"><div class="label-item"> <i class="fa fa-clock-o"></i><span class="date">2021-10-28</span></div><div class="label-item"> <i class="fa fa-list"></i><a class="tag" href="/categories/Pytorch/" title="Pytorch">Pytorch</a></div><div class="label-item"><i class="fa fa-bookmark"></i><a class="tag" href="/tags/DL/" title="DL">DL</a><i class="fa fa-bookmark"></i><a class="tag" href="/tags/pytorch/" title="pytorch">pytorch</a></div><span class="leancloud_visitors"></span></div><div class="post-content"><h1>Tips</h1>
<p><code>var.item()</code> - 如果张量 var 中只有一个元素，使用该方法取出该元素</p>
<h1>属性</h1>
<p><code>tensor.size</code> - shapes</p>
<h1>tensor的拼接</h1>
<p>tensor的拼接操作经常用到、特地将其整理如下</p>
<ul>
<li>cat - dim=all</li>
<li>stack - dim=new_dim</li>
<li>dstack - dim=2</li>
<li>hstack - dim=1</li>
<li>vstack- dim=0</li>
</ul>
<h2 id="cat">cat</h2>
<p><strong>功能</strong>：将给定的 tensors 在给定的维度上拼接</p>
<p><strong>注意</strong>：除了拼接的维度其余维度大小应一致</p>
<p><strong>参数</strong>：</p>
<ul>
<li>
<p>tensors - 多个tensor组成的元组（序列）</p>
</li>
<li>
<p>dim - 想要拼接的维度</p>
</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; torch.randn(2, 3, 3)
y &#x3D; torch.cat((x, x, x), dim&#x3D;0)
y_1 &#x3D; torch.cat((x, x, x), dim&#x3D;1)
y_2 &#x3D; torch.cat((x, x, x), dim&#x3D;2)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">x.size() &#x3D; torch.Size([2, 3, 3])
y.size() &#x3D; torch.Size([6, 3, 3])
y_1.size() &#x3D; torch.Size([2, 9, 3])
y_2.size() &#x3D; torch.Size([2, 3, 9])</code></pre>
<hr>
<h2 id="stack">stack</h2>
<p><strong>功能</strong>：将给定的 tensors 在新的维度进行拼接</p>
<p><strong>注意</strong>：每一个 tensor 的大小都应一致</p>
<p><strong>参数</strong>：tensors</p>
<ul>
<li>dim - 所要插入的维度，最小为 0 ，最大为输入数据的维度值</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.zeros(3, 4)
b &#x3D; torch.ones(3, 4)
c &#x3D; torch.stack((a, b))
d &#x3D; torch.stack((a, b), dim&#x3D;0)
e &#x3D; torch.stack((a, b), dim&#x3D;1)
f &#x3D; torch.stack((a, b), dim&#x3D;2)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">c.size() &#x3D; torch.Size([2, 3, 4])
d.size() &#x3D; torch.Size([2, 3, 4])
e.size() &#x3D; torch.Size([3, 2, 4])
f.size() &#x3D; torch.Size([3, 4, 2])</code></pre>
<hr>
<h2 id="dstack">dstack</h2>
<p><strong>功能</strong>：将给定的 tensors 沿着深度（depth）方向 (dim=2) 叠加</p>
<p>注意：其余维度大小一致</p>
<p><strong>参数</strong>：tensors</p>
<p><strong>Test 1</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(2, 3, 4, 2)
b &#x3D; torch.randn(2, 3, 4, 2)
c &#x3D; torch.dstack((a, b))</code></pre>
<p>Output 1</p>
<pre class="language-python" data-language="python"><code class="language-python">c.size() &#x3D; torch.Size([2, 3, 8, 2])</code></pre>
<p><strong>Test 2 如果输入数据小于三维呢？</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(2)
b &#x3D; torch.randn(2)
c &#x3D; torch.dstack((a, b))</code></pre>
<p>Output 2 会扩增维度</p>
<pre class="language-python" data-language="python"><code class="language-python">c.size() &#x3D; torch.Size([1, 2, 2])</code></pre>
<hr>
<h2 id="hstack">hstack</h2>
<p><strong>功能</strong>：将给定的 tensors 沿着水平（horizontal）方向 (dim=1) 叠加</p>
<p><strong>注意</strong>：其余维度大小一致</p>
<p><strong>参数</strong>：tensors</p>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(2, 3, 4)
b &#x3D; torch.randn(2, 4, 4)
c &#x3D; torch.hstack((a, b))
d &#x3D; torch.cat((a, b), dim&#x3D;1)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">c.size() &#x3D; torch.Size([2, 7, 4])
d.size() &#x3D; torch.Size([2, 7, 4])</code></pre>
<hr>
<h2 id="vstack">vstack</h2>
<p><strong>功能</strong>：将给定的 tensors 沿着竖直（vertical）方向 (dim=0) 叠加</p>
<p><strong>注意</strong>：其余维度大小一致</p>
<p><strong>参数</strong>：tensors</p>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(2, 3, 4)
b &#x3D; torch.randn(3, 3, 4)
c &#x3D; torch.vstack((a, b))
d &#x3D; torch.cat((a, b), dim&#x3D;0)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">c.size() &#x3D; torch.Size([5, 3, 4])
d.size() &#x3D; torch.Size([5, 3, 4])</code></pre>
<h1>tensor的切分</h1>
<p>数据的预处理以及数据集的构建会经常使用到tensor的切分操作，现整理如下：</p>
<ul>
<li>chunk</li>
<li>split</li>
<li>numpy中的切片(slice)操作</li>
<li>unbind</li>
</ul>
<hr>
<h2 id="chunk">chunk</h2>
<p><strong>功能</strong>：输入数据与想要切分的块数 chunks ，将数据尽可能 (如果数据个数与块数能整除的话) 平均的切分为 chunks 块</p>
<p><strong>注意</strong>：没有进行数据的拷贝</p>
<p><strong>参数</strong></p>
<ul>
<li>input - tensor</li>
<li>chunks - 切分的块数</li>
<li>dim - int - 切分的维度</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-Python" data-language="Python"><code class="language-Python">x &#x3D; torch.randn(33, 16)
y &#x3D; torch.chunk(x, 4, dim&#x3D;0)
y_1 &#x3D; torch.chunk(x, 2, dim&#x3D;1)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python"># 尽可能平均的切分为了四块
y.size() &#x3D; (torch.Size([9, 16]),
			torch.Size([9, 16]),
			torch.Size([9, 16]),
			torch.Size([6, 16]))
y_1.size() &#x3D; (torch.Size([33, 8]),
			  torch.Size([33, 8]))</code></pre>
<hr>
<h2 id="split">split</h2>
<p><strong>功能</strong>：</p>
<ul>
<li>可以输入整型数据 size 表示将数据尽可能 按照大小 size 进行分割，自行计算分割的块数</li>
<li>可以输入序列数据 list 将数据分割为 len(list) 块，每块的大小对应于list中的值</li>
</ul>
<p><strong>注意</strong>：没有对输入数据进行拷贝</p>
<p><strong>参数</strong>：</p>
<ul>
<li>input - tensor</li>
<li>split_size_or_sections - int or (ints) - 详见功能介绍</li>
<li>dim - int - 切分的维度</li>
</ul>
<p><strong>Test 1 输入整型数据</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.arange(10).view(2, 5)
# 在维度 1 上进行切分，使得每一块大小尽可能为 2
b &#x3D; torch.split(a, 2, dim&#x3D;1)</code></pre>
<p>Output 1</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
b &#x3D; (tensor([[0, 1],
            [5, 6]]), 
     tensor([[2, 3],
            [7, 8]]), 
     tensor([[4],
             [9]])) # 最后剩余 - 无法按照size大小进行分配</code></pre>
<p><strong>Test 2 输入为整型序列</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.arange(10).view(2, 5)
# 在维度 0 上进行切分，目标切分为 2 块，第一块维度大小为 1，第二块维度大小为 1
b &#x3D; torch.split(a, [1, 1], dim&#x3D;0)</code></pre>
<p>Output</p>
<pre class="language-pyhon" data-language="pyhon"><code class="language-pyhon">a &#x3D; tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
b &#x3D; (tensor([[0, 1, 2, 3, 4]]), 
     tensor([[5, 6, 7, 8, 9]]))</code></pre>
<hr>
<h2 id="slice">slice</h2>
<p><strong>功能</strong>：torch支持numpy中对数据的切片操作</p>
<p><strong>Test</strong></p>
<pre class="language-Python" data-language="Python"><code class="language-Python">x &#x3D; torch.randn(3, 2)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; tensor([[-0.2893, -1.1715],
            [-0.2477, -2.7052],
            [-0.2827, -0.2669]])
x[:,1] &#x3D; tensor([-1.1715, -2.7052, -0.2669])
x[0,:] &#x3D; tensor([-0.2893, -1.1715])
x[1:,-1]tensor([-2.7052, -0.2669])</code></pre>
<hr>
<h2 id="unbind">unbind</h2>
<p><strong>功能</strong>：删除tensor的一个维度，返回各个子块组成的 tuple</p>
<p><strong>参数</strong>：</p>
<ul>
<li>input - tensor</li>
<li>dim - int - 想要删除的维度</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randint(-5, 5, (2, 3, 4))
b &#x3D; torch.unbind(a, 1)</code></pre>
<p>Output</p>
<pre class="language-Python" data-language="Python"><code class="language-Python"># a
tensor([[[ 2, -5, -5,  4],
         [ 4, -4,  4, -5],
         [-4, -4,  1, -4]],

        [[ 1, -2,  2, -4],
         [-2, -3, -2,  1],
         [ 1, -2,  2, -2]]])
# b
tensor([[ 2, -5, -5,  4],
        [ 1, -2,  2, -4]])
tensor([[ 4, -4,  4, -5],
        [-2, -3, -2,  1]])
tensor([[-4, -4,  1, -4],
        [ 1, -2,  2, -2]])</code></pre>
<hr>
<h1>tensor的索引</h1>
<p>根据索引获取数据中特定位置的值</p>
<ul>
<li>gather</li>
<li>index_select</li>
<li>masked_select</li>
<li>narrow</li>
<li>nonzero</li>
<li>where</li>
<li>take</li>
</ul>
<hr>
<h2 id="gather">gather</h2>
<p><strong>功能</strong>：沿指定的维度收集值</p>
<p><strong>注意</strong>：除了指定的维度、其余维度都遍历一遍</p>
<p><strong>参数</strong>：</p>
<ul>
<li>input - tensor</li>
<li>dim - int - 收集的维度</li>
<li>index - LongTensor - 要收集的元素的索引，维度数和 input 一致</li>
<li>sparse_grad - bool - 如果 input 为 sparse tensor 那么设定为 true</li>
</ul>
<p>$$<br>
out[i][j][k] = input[\ index[i][j][k]\ ][j][k]\ with\ dim=0 \<br>
out[i][j][k] = input[i][\ index[i][j][k]\ ][k]\ with\ dim=1 \<br>
out[i][j][k] = input[i][j][\ index[i][j][k]\ ]\ with\ dim=2 \<br>
$$</p>
<p>**Test 1 **</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.arange(6).view(2, 3)
b &#x3D; torch.gather(a, 1, torch.tensor([
    [0, 1, 2],
    [1, 2 ,0],
])) # 确定维度后线性索引即可</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; tensor([[0, 1, 2],
            [3, 4, 5]])
b &#x3D; tensor([[0, 1, 2],
            [4, 5, 3]])</code></pre>
<h2 id="index-select">index_select</h2>
<p><strong>功能</strong>：在指定的维度上根据索引取值</p>
<p><strong>注意</strong>：进行了拷贝</p>
<p><strong>参数</strong>：</p>
<ul>
<li>input - tensor</li>
<li>dim - int</li>
<li>index - LongTensor - 一维 tensor 指定索引</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.arange(6).view(2, 3)
b &#x3D; torch.index_select(a, 1, torch.tensor([0, 2]))</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; tensor([[0, 1, 2],
            [3, 4, 5]])
b &#x3D; tensor([[0, 2],
            [3, 5]])</code></pre>
<h2 id="masked-select">masked_select</h2>
<p><strong>功能</strong>：根据 booltensor 进行索引，将索引到的值以一维 tensor 的形式返回</p>
<p><strong>注意</strong>：boolTensor 支持广播</p>
<p><strong>参数</strong>：</p>
<ul>
<li>input - tensor</li>
<li>mask - boolTensor</li>
</ul>
<p><strong>Test 1</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.arange(12).view(3, 4)
# ge - great_equal
mask &#x3D; a.ge(6) 
b &#x3D; torch.masked_select(a, mask)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; tensor([[False, False, False, False],
            [False, False,  True,  True],
            [ True,  True,  True,  True]])
tensor([ 6,  7,  8,  9, 10, 11])</code></pre>
<p><strong>Test 2 广播测试</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.arange(12).view(3, 4)
b &#x3D; torch.masked_select(a, torch.tensor([[True], [False], [False]])) # 可以广播</code></pre>
<p>Output 2</p>
<pre class="language-Python" data-language="Python"><code class="language-Python">a &#x3D; tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])
b &#x3D; tensor([0, 1, 2, 3])</code></pre>
<h2 id="narrow">narrow</h2>
<p><strong>功能</strong>：类似于 numpy中的切片操作，针对某一维度进行索引，（连续）取值</p>
<p><strong>参数</strong>：</p>
<ul>
<li>
<p>input - tensor</p>
</li>
<li>
<p>dim - int - 所要操作的维度</p>
</li>
<li>
<p>start - int - 开始的索引</p>
</li>
<li>
<p>length - 保留长度</p>
</li>
</ul>
<p><strong>简单理解：</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">torch.narrow(input, dim&#x3D;1, start, length) &#x3D; input[:,start:start+length,:,:...]</code></pre>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; torch.arange(12).view(2, 2, 3)
y &#x3D; torch.narrow(x, 2, 1, 2)
z &#x3D; x[:,:,1:3]</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; tensor([[[ 0,  1,  2],
             [ 3,  4,  5]],
 
            [[ 6,  7,  8],
             [ 9, 10, 11]]])
y &#x3D; tensor([[[ 1,  2],
             [ 4,  5]],

            [[ 7,  8],
             [10, 11]]])
z &#x3D; tensor([[[ 1,  2],
             [ 4,  5]],

            [[ 7,  8],
             [10, 11]]])</code></pre>
<h2 id="nonzero">nonzero</h2>
<p><strong>功能</strong>：找出给定 tensor 中非零元素的索引</p>
<p><strong>注意</strong>：as_tuple 参数与输出样式有关</p>
<ul>
<li>True - 返回一个二维 tensor，每一行表示一个非零值的索引</li>
<li>False - 返回 n 个大小为 z 的一维 tensor - n 表示输入数据的维度 - z表示非零元素的个数</li>
</ul>
<p><strong>参数</strong></p>
<ul>
<li>input - tensor</li>
<li>as_tuple - default (False)</li>
</ul>
<p><strong>Test 1</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randint(0, 2, (3, 3))
i_2D &#x3D; torch.nonzero(a)
i_1D &#x3D; torch.nonzero(a, as_tuple&#x3D;True)</code></pre>
<p>Output 1</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; tensor([[0, 0, 0],
            [0, 0, 1],
            [0, 0, 1]])
i_2D &#x3D; tensor([[1, 2],
               [2, 2]])
i_1D &#x3D; (tensor([1, 2]), tensor([2, 2]))</code></pre>
<p><strong>扩展</strong>：</p>
<p>结合 torch 中类似 numpy 的 bool索引，我们可以构造条件索引的形式</p>
<ul>
<li>通过条件语句生成 bool tensor</li>
<li>True为1，False为0，即nonzero会找到所有Ture值的索引（条件筛选出来的元素）</li>
</ul>
<p><strong>Test 2 利用条件语句实现逆 nonzero</strong></p>
<pre class="language-Python" data-language="Python"><code class="language-Python">a &#x3D; torch.randint(0, 2, (3, 3))
bool_tensor &#x3D; (a &#x3D;&#x3D; 0)
index_nonzero &#x3D; torch.nonzero(bool_type)</code></pre>
<p>Output 2</p>
<pre class="language-python" data-language="python"><code class="language-python"># a
tensor([[1, 0, 1],
        [1, 1, 1],
        [1, 0, 1]])
# bool_tensor
tensor([[False,  True, False],
        [False, False, False],
        [False,  True, False]])
# index_nonzero
tensor([[0, 1],
        [2, 1]])</code></pre>
<p><strong>Test 3 如何合并多个条件呢</strong></p>
<p>利用集合操作即可</p>
<ul>
<li>
<p>根据原子条件分别生成多个 booltensor</p>
</li>
<li>
<p>根据需求采用交或并等的方式合并多个 booltensor</p>
</li>
</ul>
<pre class="language-Python" data-language="Python"><code class="language-Python">a &#x3D; torch.randint(0, 8, (2, 5)) # 1 &lt; a &lt; 4: a &#x3D; 2 or 3
bool_tensor_1 &#x3D; a &gt; 1
bool_tensor_2 &#x3D; a &lt; 4
bool_tensor &#x3D; bool_tensor_1 &amp; bool_tensor_2 # 交集
index &#x3D; torch.nonzero(bool_tensor)</code></pre>
<p>Output 3</p>
<pre class="language-Python" data-language="Python"><code class="language-Python"># a
tensor([[1, 4, 2, 6, 0],
        [5, 3, 1, 2, 6]])
# bool tensor 1
tensor([[False,  True,  True,  True, False],
        [ True,  True, False,  True,  True]])
# bool tensor 2
tensor([[ True, False,  True, False,  True],
        [False,  True,  True,  True, False]])
# bool tensor
tensor([[False, False,  True, False, False],
        [False,  True, False,  True, False]])
# index
tensor([[0, 2],
        [1, 1],
        [1, 3]])</code></pre>
<h2 id="where">where</h2>
<p><strong>功能 1</strong>：根据 booltensor 返回目标值的索引</p>
<p><strong>简单理解</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">torch.where(condition) &#x3D;&#x3D; torch.nonzero(condition, as_tuple&#x3D;True)</code></pre>
<p><strong>参数</strong>：condition - booltensor</p>
<p><strong>Test 1</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; torch.randn(3, 2)
y &#x3D; torch.where(x &gt; 0)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; tensor([[ 1.2484, -0.4649],
            [ 1.7428, -0.3610],
            [ 0.3865, -1.1223]])
y &#x3D; (tensor([0, 1, 2]), tensor([0, 0, 0]))</code></pre>
<p><strong>功能 2</strong>：根据 booltensor 定位到目标值，选择性进行 值替换</p>
<p><strong>注意</strong>：支持广播</p>
<p><strong>简单理解</strong><br>
$$<br>
out_i =<br>
\begin{cases}<br>
x_i \ &amp;if\ condition_i \<br>
y_i \ &amp;otherwise<br>
\end{cases}<br>
$$<br>
<strong>参数</strong>：</p>
<ul>
<li>condition - booltensor</li>
<li>x - tensor</li>
<li>y - tensor</li>
</ul>
<p><strong>Test 2 大于0不变 小于等于0改变</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; torch.randn(3, 2)
y &#x3D; torch.ones(3, 2)
z &#x3D; torch.where(x &gt; 0, x, y)
z_1 &#x3D; torch.where(x &gt; 0, x, torch.tensor([[1.], 
                                          [2.], 
                                          [3.]])) # 支持广播</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; tensor([[-1.9040, -0.3891],
            [ 1.0645, -1.0039],
            [-0.1270, -0.2566]])
z &#x3D; tensor([[1.0000, 1.0000],
            [1.0645, 1.0000],
            [1.0000, 1.0000]])
# z_1
tensor([[1.0000, 1.0000],
        [1.0645, 2.0000],
        [3.0000, 3.0000]])</code></pre>
<h2 id="take">take</h2>
<p><strong>功能</strong>：根据一维索引进行取值，返回一维tensor （内部将输入的tensor铺平后进行索引）</p>
<p><strong>功能</strong>：根据一维索引进行取值，返回一维tensor （内部将输入的tensor铺平后进行索引）</p>
<p><strong>注意</strong>：进行了数据拷贝</p>
<p><strong>参数</strong></p>
<ul>
<li>input</li>
<li>indices - 一维索引</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; torch.arange(6).view(2, 3)
indices &#x3D; torch.randint(6, (3, ))
y &#x3D; torch.take(x, indices)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; tensor([[0, 1, 2],
            [3, 4, 5]])
indices &#x3D; tensor([0, 5, 2])
y &#x3D; tensor([0, 5, 2])</code></pre>
<h1>tensor 的填充</h1>
<p>ref：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_">https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_</a></p>
<h2 id="scatter">scatter</h2>
<p>scatter 是撒播的意思，希望能先入为主的理解一下这个函数的功能，就是往目标矩阵撒一些数值进去，往哪撒（dim？index？）与撒什么值（src）将由我们输入的参数决定</p>
<p>官方一些的说明：将 src 中数据根据 index 中的索引按照 dim 的方向填进 input 中</p>
<pre class="language-python" data-language="python"><code class="language-python">tensor.scatter(dim, index, src)</code></pre>
<p>从数学公式上来看执行流程：</p>
<pre class="language-python" data-language="python"><code class="language-python">self[index[i][j][k]][j][k] &#x3D; src[i][j][k]  # if dim &#x3D;&#x3D; 0
self[i][index[i][j][k]][k] &#x3D; src[i][j][k]  # if dim &#x3D;&#x3D; 1
self[i][j][index[i][j][k]] &#x3D; src[i][j][k]  # if dim &#x3D;&#x3D; 2</code></pre>
<p>1、原始的对应填充方式 <code>self[i][j][k] = src[i][j][k]</code> 将被 <code>index</code> 索引张量影响，因此才能体现出撒（scatter）的感觉</p>
<p>2、dim 决定影响的维度，其余维度不变</p>
<p>3、我们对 index 进行遍历时，dim 影响的那一个维度会由 index 内部的值决定，其余维度索引保持一致</p>
<p>希望以上三点能加深对下面例子的理解</p>
<blockquote>
<p>为了保证索引对应 index 索引张量的维度应与 src 张量的维度一致，但是 shape 可以不一致</p>
</blockquote>
<p>让我们以一个二维的情形为例：</p>
<p><img src="https://gitee.com/Butterflier/pictures/raw/master/image-20211028195119040.png" alt="image-20211028195119040"></p>
<p>从 index 出发，因为 dim = 1，所以我们能够得知目标填充索引为 (0, 3)</p>
<blockquote>
<p>可以认为先从 (i = 0, j=0) 找到了 index 和 value 的值（3, 9），又因为 dim = 1，因此 index 的值将作用于列索引所以得到最终索引为 (i=0, j=3)</p>
</blockquote>
<p>其实再次看这句话</p>
<blockquote>
<p>我们对 index 进行遍历时，dim 影响的那一个维度会由 index 内部的值决定，其余维度索引保持一致</p>
</blockquote>
<p>那我们看 value 中的 5，它的行索引是 1，因此它必然撒到 dest 中的第一行（dim=1），但是撒在第几列将由 index 决定，index 说撒在第 4 列，done！</p>
<p><img src="https://gitee.com/Butterflier/pictures/raw/master/image-20211028195554262.png" alt="image-20211028195554262"></p>
<p><strong>Example： Lable 转 one-hot</strong></p>
<p>Label 通常是一维的 <code>shape=(N,)</code>，而模型的 output 通常是 softmax 之后的二维结果<code>shape=(N, dim)</code>，因此将 label 变为 二维的 one-hot 形式非常常见</p>
<p>首先指定 dim = 1，是因为我们填充值要修改的维度索引是列向维度，行索引值将与 Label 的索引值对应，即通过控制列向维度保证哪个地方值是 1</p>
<pre class="language-python" data-language="python"><code class="language-python"># ref: https:&#x2F;&#x2F;www.cnblogs.com&#x2F;dogecheng&#x2F;p&#x2F;11938009.html
class_num &#x3D; 10
batch_size &#x3D; 4
label &#x3D; torch.LongTensor(batch_size, 1).random_() % class_num
#tensor([[6],
#        [0],
#        [3],
#        [2]])
torch.zeros(batch_size, class_num).scatter_(1, label, 1)
#tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
#        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
#        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
#        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])</code></pre>
<p>label 即是我们要索引的列，我们需要完成对 label 的遍历</p>
<p>1、第一个值为 <code>6</code> 告知第一行第 <code>6</code> 列</p>
<p>1、第二个值为 <code>0</code> 告知第二行第 <code>0</code> 列</p>
<p>…</p>
<h1>tensor的变换</h1>
<ul>
<li>movedim</li>
<li>reshape</li>
<li>view</li>
<li>squeeze</li>
<li>unsqueeze</li>
<li>t</li>
<li>transpose</li>
</ul>
<h2 id="movedim">movedim</h2>
<p><strong>功能</strong>：进行（多个）维度的移动</p>
<p><strong>注意</strong>：其余维度保持原有不变，并按照原顺序依次落位</p>
<p><strong>参数</strong></p>
<ul>
<li>input</li>
<li>source - int or tuple - 维度移动前的索引</li>
<li>destination - int or tuple - 维度移动后的索引</li>
</ul>
<p><strong>Test 1 其余维度保持原有不变，并按照原顺序依次落位</strong></p>
<pre class="language-Python" data-language="Python"><code class="language-Python">a &#x3D; torch.randn(2, 3, 4, 5)
b &#x3D; torch.movedim(a, 0, 3)
c &#x3D; torch.movedim(a, 0, 2)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.Size([2, 3, 4, 5])
b &#x3D; torch.Size([3, 4, 5, 2])
c &#x3D; torch.Size([3, 4, 2, 5])</code></pre>
<p><strong>Test 2 输入为tuple</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(2, 3, 4, 5)
b &#x3D; torch.movedim(a, (1, 2), (1, 0))</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.Size([2, 3, 4, 5])
b &#x3D; torch.Size([4, 3, 2, 5])</code></pre>
<h2 id="reshape">reshape</h2>
<p><strong>功能</strong>：改变输入 tensor 的维度分配</p>
<p><strong>注意</strong>：应默认该函数不会对源数据进行拷贝</p>
<p><strong>参数</strong></p>
<ul>
<li>input</li>
<li>shape - int of tuple - 其余维度确定的情况下，可以使用 -1 用于推算某一维度的大小</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(32, 16)
b &#x3D; torch.reshape(a, (4, 2, 2, -1 ,2))</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.Size([32, 16])
b &#x3D; torch.Size([4, 2, 2, 16, 2])</code></pre>
<h2 id="view">view</h2>
<h2 id="squeeze">squeeze</h2>
<p><strong>功能</strong>：默认移除 tensor 中维度大小为 1 的维度 - 给定 dim 后只考虑给定的 dim 是否移除</p>
<p><strong>注意</strong>：输入 dim 必须为整型，不能为整型序列</p>
<p><strong>参数</strong>：</p>
<ul>
<li>input - tensor</li>
<li>dim - default(None) - int</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(2, 1, 2, 1, 1, 3)
b &#x3D; torch.squeeze(a)
c &#x3D; torch.squeeze(a, dim&#x3D;1)
d &#x3D; torch.squeeze(a, dim&#x3D;2)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">torch.Size([2, 1, 2, 1, 1, 3])
torch.Size([2, 2, 3])
torch.Size([2, 2, 1, 1, 3])
torch.Size([2, 1, 2, 1, 1, 3])</code></pre>
<h2 id="unsqueeze">unsqueeze</h2>
<p><strong>功能</strong>：与 squeeze 方法想反，增加一个大小为1的维度</p>
<p><strong>参数</strong></p>
<ul>
<li>input - tensor</li>
<li>dim - int</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; torch.tensor([1, 2, 3, 4])
y &#x3D; torch.unsqueeze(x, dim&#x3D;0)
z &#x3D; torch.unsqueeze(x, dim&#x3D;1)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">x &#x3D; torch.Size([4])
y &#x3D; torch.Size([1, 4])
z &#x3D; torch.Size([4, 1])</code></pre>
<h2 id="t">t</h2>
<p><strong>功能</strong>：对二维张量进行转置处理</p>
<p><strong>注意</strong>：对于一维 tensor 返回的是它本身</p>
<p><strong>参数</strong>：input - tensro</p>
<p><strong>Test</strong></p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.randn(2)
a_t &#x3D; torch.t(a)
b &#x3D; torch.randn(2, 3)
b_t &#x3D; b.t()</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">  a &#x3D; torch.Size([2])
a_t &#x3D; torch.Size([2])
  b &#x3D; torch.Size([2, 3])
b_t &#x3D; torch.Size([3, 2])</code></pre>
<h2 id="transpose">transpose</h2>
<p><strong>功能</strong>：将给定的两个维度进行交换</p>
<p><strong>参数</strong>：</p>
<ul>
<li>input - tensor</li>
<li>dim0 - int</li>
<li>dim1 - int</li>
</ul>
<p><strong>Test</strong></p>
<pre class="language-Python" data-language="Python"><code class="language-Python">a &#x3D; torch.randn(2, 3, 4, 5)
b &#x3D; torch.transpose(a, 1, 2)</code></pre>
<p>Output</p>
<pre class="language-python" data-language="python"><code class="language-python">a &#x3D; torch.Size([2, 3, 4, 5])
b &#x3D; torch.Size([2, 5, 4, 3])</code></pre>
<h1>与Numpy转换</h1>
<p>Tips：Torch Tensor 与 Numpy array 共享底层的内存空间</p>
<p>Tips：所有在 CPU 上的 tensor （except CharTensor）都可与 Numpy array 互转换</p>
<p><code>to numpy</code>：<code>tensor.numpy()</code></p>
<p><code>to tensor</code> ：<code>torch.from_numpy(array)</code></p>
<h1>与设备交互</h1>
<pre class="language-python" data-language="python"><code class="language-python">if torch.cuda.is_available():
    device &#x3D; torch.device(&quot;cuda&quot;)

gpu_tensor &#x3D; torch.ones_like(x, device&#x3D;device)
gpu_tensor &#x3D; cpu_tensor.to(device)
cpu_tensor &#x3D; gpu_tensor.to(&quot;cpu&quot;, torch.double)</code></pre>
<h1>自动求导</h1>
<p>定义变量时设定：<code>torch.tensor([1, 2, 3], requires_grad=True)</code></p>
<p>后续更改设定：<code>tensor.requires_grad_(True)</code></p>
<p>终止某Tensor在计算图中的追踪回溯：<code>tensor.detach()</code></p>
<p>终止整个流程的反向传播：<code>with torch.no_grad():</code></p>
<h2 id="相关属性">相关属性</h2>
<p><code>.grad_fn</code> - 代表引用了那个 Function 创建了该 Tensor，自定义则值为 None</p>
<p><code>.grad</code> - 在该 Tensor 上的所有梯度将被累加进该属性</p>
<h1>END</h1>
</div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://jiancongcui.github.io/2021/10/28/torch-scatter/,Jiancong Cui,torch.scatter,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2021/10/29/Same-Origin-Policy/" title="Same Origin Policy">prev_post</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2021/10/10/MAC-Spoofing-and-Vlan-Defense/" title="MAC Spoofing and Vlan Defense">next_post</a></li></ul></div></div></div><div class="footer animated fadeInDown"><div class="p"> <span>© 2024 - 2029 </span><i class="fa fa-star"></i><span> JCC</span></div><div class="by_farbox"><span>Powered by </span><a href="https://sites.google.com/new" target="_blank">Google Sites </a><span> & </span><a href="https://github.com/Ben02/hexo-theme-Anatole" target="_blank">Ben </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core </a></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>